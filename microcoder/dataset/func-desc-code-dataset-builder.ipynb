{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset for fine-tuning models\n",
    "\n",
    "This dataset is built for fine-tuning models on the task of code generation based on function descriptions in the field of embedded-systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import regex as re\n",
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate checkpoint .parquet files from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset by concatenating all the parquet files\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith('.parquet') and 'big_dataset_extraction_' in file:\n",
    "        print(f'Loading {file}')\n",
    "        df = pd.concat([df, pd.read_parquet(file)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_info(row):\n",
    "    \"\"\"\n",
    "    Extracts parameters, docstring, and return information from a function's description.\n",
    "\n",
    "    Args:\n",
    "        row (Series): A row from a DataFrame containing the function description.\n",
    "    \"\"\"\n",
    "    description = row['description']\n",
    "    parameters = '\\n'.join(f'@{param[\"kind\"]} {param[\"name\"]} {param[\"description\"]}' \n",
    "                           for param in description['parameters'] \n",
    "                           if all(param[key] is not None for key in ['kind', 'name', 'description']))\n",
    "\n",
    "    docstring = ''.join(filter(None, [description.get('brief'), description.get('detailed')]))\n",
    "    return_val = description.get('return', None)\n",
    "\n",
    "    return pd.Series([parameters if parameters else None, docstring if docstring else None, return_val])\n",
    "\n",
    "# Apply the function and update the DataFrame\n",
    "df[['parameters', 'docstring', 'return']] = df.apply(extract_function_info, axis=1)\n",
    "\n",
    "# Drop the original 'description' column\n",
    "df.drop(columns=['description'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column repository\n",
    "df['repository'] = df['file'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "# Add column language\n",
    "df['language'] = df['file'].apply(lambda x: 'C' if x.split('/')[1].endswith('.c') else 'C++')\n",
    "\n",
    "# Drop rows, where value in column 'code' and 'docstring' is NaN\n",
    "df.dropna(inplace=True, subset=['code', 'docstring'])\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['signature', 'code'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Sort by repository\n",
    "df = df.sort_values(by='repository', ascending=True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset (checkpoint)\n",
    "df.to_parquet('combined_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up dataset for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('combined_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count of instances of eacg keyword in docstring and code\n",
    "keywords = ['TODO', 'FIXME', 'HACK', 'BUG', 'ISSUE', 'REVIEW', 'REFACTOR', 'DEPRECATED', 'OBSOLETE', 'JOKE', 'TEST', 'COPYRIGHT', 'TOKEN', 'HTTP', '#IFDEF', '#IFNDEF', '#ENDIF', '#UNDEF', '#IF', '#ELSE', '#ENDIF', 'LICENSE']\n",
    "\n",
    "keywords_pattern = '|'.join(keywords)\n",
    "pattern = re.compile(keywords_pattern, re.IGNORECASE)\n",
    "\n",
    "# Count of instances of each keyword in docstring and code\n",
    "for keyword in keywords:\n",
    "    print(f\"{keyword} in code: {df['code'].str.count(keyword).sum()}, in docstring: {df['docstring'].str.count(keyword).sum()}\")\n",
    "\n",
    "# Filter rows based on regex pattern\n",
    "mask =  df.apply(lambda x: False if (pattern.search(x['code']) or pattern.search(x['docstring'])) is None else True, axis=1)\n",
    "df = df[~mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.items.pop()\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.items) == 0\n",
    "\n",
    "def check_braces(file_data) -> bool:\n",
    "    stack = Stack()\n",
    "    for char in file_data:\n",
    "        if char == '{':\n",
    "            stack.push('{')\n",
    "        elif char == '}':\n",
    "            if stack.is_empty():\n",
    "                return 'Braces are invalid'  # Too many closing braces\n",
    "            stack.pop()\n",
    "    \n",
    "    if stack.is_empty():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def clean_code(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the code by removing trailing and leading content outside the main code block, \n",
    "    one line comments, unnecessary newlines, and empty lines.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string containing the code.\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned code.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sometimes, the extarction fails and wrong code blocks are extracted, for example when the code block is not enclosed in braces or there is some text before the code block.\n",
    "    # First we need to check if the code block is enclosed in braces, if not, we will remove the code block.\n",
    "    # If the code block is enclosed in braces, we will remove the text before the code block.\n",
    "    if not check_braces(text):\n",
    "        return None\n",
    "    else:\n",
    "        text = text[text.find('{'):text.rfind('}') + 1]\n",
    "\n",
    "    # Stop if the text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Remove comments /* text */ from code\n",
    "    text = re.sub(r'[\\s]*\\/\\*[^\\*]*(?:\\*\\/)+', '', text, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    # Remove comments // text from code\n",
    "    text = re.sub(r'[\\s]*\\/\\/[^\\n]*', '', text, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    # Remove multiline comments\n",
    "    text = re.sub(r'\\/\\*[\\s\\S]*?\\*\\/', '', text)\n",
    "\n",
    "    # Remove double newlines and similar patterns\n",
    "    text = re.sub(r'\\t+\\n', '', text)\n",
    "    text = re.sub(r'\\n *\\n', '\\n', text)\n",
    "    text = re.sub(r'^\\s*\\n', '', text, flags=re.MULTILINE) # Remove empty lines\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function for code\n",
    "df['code'] = df['code'].apply(clean_code)\n",
    "\n",
    "# Remove rows with empty code\n",
    "df.replace({'code': ''}, np.nan, inplace=True)\n",
    "df.dropna(subset=['code'], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "invalid_counter = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if not check_braces(df.loc[i, 'code']):\n",
    "        invalid_counter += 1\n",
    "        df.loc[i, 'code'] = None\n",
    "\n",
    "df.dropna(subset=['code'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of invalid code blocks removed: {invalid_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the description by removing unnecessary content from description\n",
    "\n",
    "    Args:\n",
    "        text (str): A string containing the description.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned description.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove dates in various formats\n",
    "    text = re.sub(r'\\d{4}-\\d{1,2}-\\d{1,2}', '', text)\n",
    "    text = re.sub(r'\\d{1,2}-\\d{1,2}-\\d{4}', '', text)\n",
    "    text = re.sub(r'\\d{1,2}-\\d{1,2}-\\d{2,4}', '', text)\n",
    "\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{4}', '', text)\n",
    "    text = re.sub(r'\\d{4}/\\d{1,2}/\\d{1,2}', '', text)\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', text)\n",
    "\n",
    "    text = re.sub(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', '', text)\n",
    "    text = re.sub(r'\\d{4}\\.\\d{1,2}\\.\\d{1,2}', '', text)\n",
    "    text = re.sub(r'\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}', '', text)\n",
    "\n",
    "    # Remove text in [] brackets, this tends to be present, but is not useful\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "    \n",
    "# Apply the cleaning function for description\n",
    "df['docstring'] = df['docstring'].apply(clean_description)\n",
    "\n",
    "df = df[df['docstring'] != ''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer. We will use the codellama/CodeLlama-7b-hf tokenizer since ths will be the model we will fine-tune.\n",
    "base_model_name = \"bigcode/starcoderbase-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, add_bos_token=False, add_eos_token=False, token='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')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with less than 8 words in docstring\n",
    "df = df[df['docstring'].apply(lambda x: len(x.split()) >= 8)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize code\n",
    "df['code_tokens'] = df['code'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df['code_tokens_len'] = df['code_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "# Tokenize docstring\n",
    "df['docstring_tokens'] = df['docstring'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df['docstring_tokens_len'] = df['docstring_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "df['n_of_lines'] = df['code'].apply(lambda x: len(x.split('\\n')))\n",
    "\n",
    "df['code_unique_tokens'] = df['code_tokens'].apply(lambda x: len(np.unique(x))) # Number of unique tokens in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by number of tokens in code and docstring\n",
    "rows = df[((df['code_tokens_len'] >= 15) & (df['code_tokens_len'] <= 256) & (df['docstring_tokens_len'] <= 100) & (df['n_of_lines'] >= 3) & (df['n_of_lines'] <= 30))].index\n",
    "new_df = df.loc[rows].reset_index(drop=True)\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_parquet('filtered_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and export it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_parquet('filtered_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docstring(docstring: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the docstring to be used in the model training.\n",
    "\n",
    "    We will use the following format:\n",
    "    /* docstring */\n",
    "    Since the same fomart is used in THUDM/humaneval-x dataset for c++ code.\n",
    "\n",
    "    Args:\n",
    "        docstring (str): The docstring to be formatted.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted docstring.\n",
    "    \"\"\"\n",
    "    docstring = re.sub(r'\\n*$', '', docstring)\n",
    "\n",
    "    return f\"/* {docstring} */\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format docstring\n",
    "training_dataset['docstring'] = training_dataset['docstring'].apply(lambda x: format_docstring(x))\n",
    "\n",
    "# Create prompt\n",
    "training_dataset['prompt'] = training_dataset['docstring'] + '\\n' + training_dataset['signature']\n",
    "\n",
    "training_dataset = training_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in signature\n",
    "training_dataset = training_dataset.drop_duplicates(subset=['signature', 'code'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Choose only examples where repository has more than 100 examples\n",
    "training_dataset = training_dataset[training_dataset['repository'].map(training_dataset['repository'].value_counts()) >= 100].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save whole dataframe\n",
    "training_dataset.to_parquet('dataset-all-features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dataset = training_dataset[['docstring', 'signature', 'prompt', 'code', 'repository', 'language']]\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation and test set\n",
    "train, validate, test = np.split(export_dataset.sample(frac=1), [int(.80*len(export_dataset)), int(.90*len(export_dataset))])\n",
    "\n",
    "# Create a dataset from the dataframe\n",
    "train_dataset = datasets.Dataset.from_pandas(train, preserve_index=False)\n",
    "val_dataset = datasets.Dataset.from_pandas(validate, preserve_index=False)\n",
    "test_dataset = datasets.Dataset.from_pandas(test, preserve_index=False)\n",
    "\n",
    "\n",
    "datasetDict = datasets.DatasetDict({\"train\":train_dataset, \"validation\":val_dataset, \"test\":test_dataset})\n",
    "\n",
    "datasetDict.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish dataset to Hugging Face Datasets Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDict.push_to_hub('xvadov01/test-dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
